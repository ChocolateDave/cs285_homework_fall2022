\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Results from PointmassEasy Environment: (a) Random exploration with epsilon-greedy; (b) Exploration with RND. State density of from the two algorithms are similar but RND unexpectedly has a denser density around the lower left corner (origin).\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{1}{Results from PointmassEasy Environment: (a) Random exploration with epsilon-greedy; (b) Exploration with RND. State density of from the two algorithms are similar but RND unexpectedly has a denser density around the lower left corner (origin).\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{1}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{1}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results from PointmassMedium Environment: (a) Random exploration with epsilon-greedy; (b) Exploration with RND. State density of from the two algorithms are similar but RND has a more uniformly distributed density than the random exploration one.\relax }}{1}{figure.caption.4}\protected@file@percent }
\newlabel{fig:2}{{2}{1}{Results from PointmassMedium Environment: (a) Random exploration with epsilon-greedy; (b) Exploration with RND. State density of from the two algorithms are similar but RND has a more uniformly distributed density than the random exploration one.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{1}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{1}{figure.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning curve from the two environments. RND reaches higher score faster than epsilon-greedy on the PointmassEasy environment, while slower on the PointmassMedium environment.\relax }}{2}{figure.caption.5}\protected@file@percent }
\newlabel{fig:3}{{3}{2}{Learning curve from the two environments. RND reaches higher score faster than epsilon-greedy on the PointmassEasy environment, while slower on the PointmassMedium environment.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Results from PointmassMedium Environment: (a) Random exploration with epsilon-greedy; (b) Exploration with RND; (c) Exploration with count-based Model. State density from epsilon-greed and count-based exploration strategies are quite similar, while the one from RND-based exploration is more uniformly distributed.\relax }}{2}{figure.caption.7}\protected@file@percent }
\newlabel{fig:4}{{4}{2}{Results from PointmassMedium Environment: (a) Random exploration with epsilon-greedy; (b) Exploration with RND; (c) Exploration with count-based Model. State density from epsilon-greed and count-based exploration strategies are quite similar, while the one from RND-based exploration is more uniformly distributed.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{2}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{2}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{2}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces (a) OOD Q-value estimation; (b) Evaluation Average Returns; and (c) Buffer Q-value estimation on DQN- and CQL-based algorithm with/without reward scale and shifts. My experiment results shows that CQL works better with the original reward with only penalties of -1, while DQN works better with scale and shifted rewards. With the original reward settings, CQL helps to prevent overestimation of OOD Q values, while it fails with scaled and shifted rewards. My explanation to this is that shifting and scaling with 1 and 100 causes rewards to be more sparse (i.e., only getting 100 when success while getting 0 at other places). This mitigate overestimation of OOD Q values for DQN, which helps it perform better.\relax }}{3}{figure.caption.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Final average evaluation return of DQN- and CQL-based exploration with different exploration steps. Surprisingly, my experiment results indicate that increasing the number of unsupervised exploration steps in return deteriorate the learning performance. And DQN-based algorithm worsen much more significantly than the CQL-based one. My explanation for this is that unsupervised exploration can actually generate a series of out-of-distribution data. Given sparse rewards, it actually makes it more difficult to learn from the exploration data. And DQN can overestimate the Q values of some OOD data, which cause a even worse performance.\relax }}{3}{table.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning curve of CQL-based algorithm on \texttt  {PointmassMedium} with different $\alpha $ settings. From its formulation, higher value of $\alpha $ promotes stronger constraints on overestimation of the OOD q-value, which results in a more conservative. Therefore, the figure shows that with smaller $\alpha $ value, the algorithm tends to reach higher score earlier, but finds it hard to maintain the achievement since it tends to overestimate some OOD states. On the contrary, a higher $\alpha $ value would cause a longer time to train, but the learning curve is more stable at the end.\relax }}{4}{figure.caption.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Learning curve of DQN- and CQL-based exploitation algorithms on (a) \texttt  {PointmassMedium} and (b) \texttt  {PointmassHard} environments. Compared to the results in Part 1, supervised exploration helps stablize the learning curve and is more effective regarding the convergence. The reason I can think of is that supervised exploration has a changing weighting of exploration vs. exploitation, which helps the algorithms to adjust its strategy across different learning period to explore more effectively.\relax }}{5}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Learning curve from \texttt  {PointmassEasy} environment: (a) supervised algorithm with different $\lambda $ settings; (b) unsupervised algorithm with different $\lambda $ settings. Explorations with supervised and unsupervised perform quite similar in this environment, which I think is partly due to the simplicity of the task. The best $\lambda $ settings for unsupervised and supervised explorations are both $\lambda =1$.\relax }}{6}{figure.caption.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Learning curve from \texttt  {PointmassMedium} environment: (a) supervised algorithm with different $\lambda $ settings; (b) unsupervised algorithm with different $\lambda $ settings. Training with supervised exploration is slightly better regarding the convergence speed. The best $\lambda $ setting for supervised and unsupervised RND are $\lambda =1$ and $\lambda =20$, respectively.\relax }}{6}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Learning curve from \texttt  {PointmassEasy} environment: (a) supervised algorithm with different $\tau $ settings; (b) unsupervised algorithm with different $\tau $ settings. Increasing $\tau $ overall benefits the training but the marginal benefits decreases.\relax }}{7}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Learning curve from \texttt  {PointmassMedium} environment: (a) supervised algorithm with different $\lambda $ settings; (b) unsupervised algorithm with different $\lambda $ settings. For supervised exploration, increasing $\tau $ overall benefits the training. But for the unsupervised exploration, increasing $\tau $ does not necessarily bring benefits. The optimal $\tau $ from my experiment is around $\tau =0.7$.\relax }}{7}{figure.caption.22}\protected@file@percent }
\gdef \@abspage@last{7}
