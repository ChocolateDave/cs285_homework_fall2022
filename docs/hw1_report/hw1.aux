\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Behavior Cloning}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Task report}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Behavior Cloning (BC) Policy's Return over 30 rollouts. In both environments, the policy distribution mean network is an two-layer MLP with a hidden size of 64. The Ant-v4 and the HalfCheetah-v4 have action sizes of 8 and 6. The number of training iteration is one for both environments with a training batch size of 100. The \texttt  {ep\_len} is 1000 and the \texttt  {eval\_batch\_size} is 30000 for both tasks. The \texttt  {seed} is 42.\relax }}{1}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:1}{{1}{1}{Behavior Cloning (BC) Policy's Return over 30 rollouts. In both environments, the policy distribution mean network is an two-layer MLP with a hidden size of 64. The Ant-v4 and the HalfCheetah-v4 have action sizes of 8 and 6. The number of training iteration is one for both environments with a training batch size of 100. The \texttt {ep\_len} is 1000 and the \texttt {eval\_batch\_size} is 30000 for both tasks. The \texttt {seed} is 42.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Hyperparameter set}{1}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Evaluation Returns vs. Sizes of Hidden Layers. With different settings of the hidden layer neurons, the neural network shows different capability of representing non-linear relationship between input observation features to output action probabilities. However, the performance is not plain linear increasing with respect to increasing hidden sizes since the complexity of the network trades off marginal performance increase with fixed training steps.\relax }}{1}{figure.caption.2}\protected@file@percent }
\newlabel{fig:1}{{1}{1}{Evaluation Returns vs. Sizes of Hidden Layers. With different settings of the hidden layer neurons, the neural network shows different capability of representing non-linear relationship between input observation features to output action probabilities. However, the performance is not plain linear increasing with respect to increasing hidden sizes since the complexity of the network trades off marginal performance increase with fixed training steps.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}DAgger}{2}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) The Number of DAgger Iterations vs. the Policy's Mean Returns on the Ant-v4 Environment. Model is trained on a two-layer MLP with a hidden size of 32. The \texttt  {train\_batch\_size}, \texttt  {eval\_batch\_size}, and \texttt  {ep\_len} are 100, 30000, and 1000, respectively. The \texttt  {seed} is 42. (b) The Number of DAgger Iterations vs. the Policy's Mean Returns on the HalfCheetah-v4 Environment. Model is trained on a two-layer MLP with a hidden size of 64. The \texttt  {train\_batch\_size}, \texttt  {eval\_batch\_size}, and \texttt  {ep\_len} are 100, 30000, and 1000, respectively. The \texttt  {seed} is 42.\relax }}{2}{figure.caption.3}\protected@file@percent }
\newlabel{fig:2}{{2}{2}{(a) The Number of DAgger Iterations vs. the Policy's Mean Returns on the Ant-v4 Environment. Model is trained on a two-layer MLP with a hidden size of 32. The \texttt {train\_batch\_size}, \texttt {eval\_batch\_size}, and \texttt {ep\_len} are 100, 30000, and 1000, respectively. The \texttt {seed} is 42. (b) The Number of DAgger Iterations vs. the Policy's Mean Returns on the HalfCheetah-v4 Environment. Model is trained on a two-layer MLP with a hidden size of 64. The \texttt {train\_batch\_size}, \texttt {eval\_batch\_size}, and \texttt {ep\_len} are 100, 30000, and 1000, respectively. The \texttt {seed} is 42.\relax }{figure.caption.3}{}}
\gdef \@abspage@last{2}
