\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Part 1: Q-Learning}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces DQN performance on \texttt  {Ms. Pac-Man} in average per-epoch return (cyan curve) and best mean return (red curve) versus number of time steps. Performance of the algorithm shows a step increasing trend. However, the average per-epoch return variances are also increasing.\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{1}{DQN performance on \texttt {Ms. Pac-Man} in average per-epoch return (cyan curve) and best mean return (red curve) versus number of time steps. Performance of the algorithm shows a step increasing trend. However, the average per-epoch return variances are also increasing.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average training per-epoch return with respect to vanilla (green curve) and double (orange curve) DQN. The double DQN successfully prevents rewards from decreasing after around $30,000$ training steps and achieves a higher score.\relax }}{2}{figure.caption.7}\protected@file@percent }
\newlabel{fig:2}{{2}{2}{Average training per-epoch return with respect to vanilla (green curve) and double (orange curve) DQN. The double DQN successfully prevents rewards from decreasing after around $30,000$ training steps and achieves a higher score.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average per-epoch training returns with respect to training time steps given different Q-function neural network structures.\relax }}{3}{figure.caption.11}\protected@file@percent }
\newlabel{fig:3}{{3}{3}{Average per-epoch training returns with respect to training time steps given different Q-function neural network structures.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Part 2: Actor-Critic}{4}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Average evaluation returns with respect to training time steps given different \texttt  {ntu} and \texttt  {ngsptu} settings. From the results, $10$ target updates along with $10$ gradient steps per target update yields the best results among the four different settings.\relax }}{4}{figure.caption.15}\protected@file@percent }
\newlabel{fig:4}{{4}{4}{Average evaluation returns with respect to training time steps given different \texttt {ntu} and \texttt {ngsptu} settings. From the results, $10$ target updates along with $10$ gradient steps per target update yields the best results among the four different settings.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average evaluation return with respect to training steps of running Actor-Crtic algorithm with \texttt  {InvertedPendulum-v4} (green) and \texttt  {HalfCheetah-v4} (orange).\relax }}{5}{figure.caption.19}\protected@file@percent }
\newlabel{fig:5}{{5}{5}{Average evaluation return with respect to training steps of running Actor-Crtic algorithm with \texttt {InvertedPendulum-v4} (green) and \texttt {HalfCheetah-v4} (orange).\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Part 3: Soft Actor-Critic}{6}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average Evaluation Returns running Soft Actor-Critic on \texttt  {InvertedPendulum-v4} (left) and \texttt  {HalfCheetah-v4} (right). Average rewards on \texttt  {HalfCheetah-v4} reaches around $300$ after 50,000 steps, and rewards on \texttt  {InvertedPendulum-v4} reaches $1,000$ after 20,000 steps. However, results varies greatly with different seed settings. I only submit the best result for my submission.\relax }}{6}{figure.caption.23}\protected@file@percent }
\newlabel{fig:6}{{6}{6}{Average Evaluation Returns running Soft Actor-Critic on \texttt {InvertedPendulum-v4} (left) and \texttt {HalfCheetah-v4} (right). Average rewards on \texttt {HalfCheetah-v4} reaches around $300$ after 50,000 steps, and rewards on \texttt {InvertedPendulum-v4} reaches $1,000$ after 20,000 steps. However, results varies greatly with different seed settings. I only submit the best result for my submission.\relax }{figure.caption.23}{}}
\gdef \@abspage@last{6}
