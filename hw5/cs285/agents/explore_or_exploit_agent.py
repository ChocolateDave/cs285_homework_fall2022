from __future__ import annotations

from typing import Any, Dict

import gym
import numpy as np
from cs285.agents.dqn_agent import DQNAgent
from cs285.critics.cql_critic import CQLCritic
from cs285.critics.dqn_critic import DQNCritic
from cs285.exploration.ucb_model import UCBModel
from cs285.exploration.rnd_model import RNDModel
from cs285.infrastructure.dqn_utils import (MemoryOptimizedReplayBuffer,
                                            Schedule)
from cs285.infrastructure.utils import normalize
from cs285.policies.argmax_policy import ArgMaxPolicy


class ExplorationOrExploitationAgent(DQNAgent):
    def __init__(self,
                 env: gym.Env,
                 agent_params: Dict[str, Any],
                 normalize_rnd: bool = True,
                 rnd_gamma: float = 0.99) -> None:
        super(ExplorationOrExploitationAgent, self).__init__(env, agent_params)

        self.replay_buffer = MemoryOptimizedReplayBuffer(size=100000,
                                                         frame_history_len=1,
                                                         float_obs=True)
        self.num_exploration_steps = agent_params['num_exploration_steps']
        self.offline_exploitation = agent_params['offline_exploitation']

        self.exploitation_critic = CQLCritic(agent_params, self.optimizer_spec)
        self.exploration_critic = DQNCritic(agent_params, self.optimizer_spec)

        if agent_params['use_ucb']:
            self.exploration_model = UCBModel(env)
        else:
            self.exploration_model = RNDModel(
                agent_params, self.optimizer_spec)

        self.explore_weight_schedule: Schedule = agent_params[
            'explore_weight_schedule']
        self.exploit_weight_schedule: Schedule = agent_params[
            'exploit_weight_schedule']

        self.actor = ArgMaxPolicy(self.exploration_critic)
        self.eval_policy = ArgMaxPolicy(self.exploitation_critic)
        self.exploit_rew_shift = agent_params['exploit_rew_shift']
        self.exploit_rew_scale = agent_params['exploit_rew_scale']
        self.eps = agent_params['eps']

        self.running_rnd_rew_std = 1
        self.normalize_rnd = normalize_rnd
        self.rnd_gamma = rnd_gamma

    def train(self,
              ob_no: np.ndarray,
              ac_na: np.ndarray,
              re_n: np.ndarray,
              next_ob_no: np.ndarray,
              terminal_n: np.ndarray) -> Dict[str, Any]:
        log = {}

        if self.t > self.num_exploration_steps:
            # TODO (Done): After exploration is over,
            # set the actor to optimize the extrinsic critic (exploitation)
            # HINT: Look at method ArgMaxPolicy.set_critic
            self.actor.set_critic(self.exploitation_critic)

        if (self.t > self.learning_starts and
            self.t % self.learning_freq == 0 and
                self.replay_buffer.can_sample(self.batch_size)):
            # Get Reward Weights
            # TODO (Done): Get the current explore reward weight and exploit
            # reward weight using the schedule's passed in (see __init__)
            # HINT: Until part 3, explore_weight = 1, and exploit_weight = 0
            explore_weight = self.explore_weight_schedule.value(self.t)
            exploit_weight = self.exploit_weight_schedule.value(self.t)

            # Run Exploration Model
            # TODO (Done): Evaluate the explor model on s' to get the bonus
            # HINT: Normalize the exploration bonus,
            # as RND values vary highly in magnitude.
            # HINT: Normalize using self.running_rnd_rew_std, and keep an
            # exponential moving average of self.running_rnd_rew_std
            # using self.rnd_gamma.
            expl_bonus = self.exploration_model.forward_np(next_ob_no)
            if self.normalize_rnd:
                expl_bonus = normalize(data=expl_bonus,
                                       mean=expl_bonus.mean(),
                                       std=self.running_rnd_rew_std)
                self.running_rnd_rew_std = \
                    self.rnd_gamma * self.running_rnd_rew_std + \
                    (1 - self.rnd_gamma) * expl_bonus.std()

            # Reward Calculations
            # TODO (Done): Calculate mixed rewards,
            # which will be passed
            # into the exploration critic
            # HINT: See doc part 3 for definition of mixed_reward
            mixed_reward = explore_weight * expl_bonus + exploit_weight * re_n

            # TODO (Done): Calculate the environment reward
            # HINT: For part 1, env_reward is just 're_n'
            #       After this, env_reward is 're_n' shifted by
            #       self.exploit_rew_shift and scaled by self.exploit_rew_scale
            env_reward = self.exploit_rew_scale * \
                (re_n + self.exploit_rew_shift)

            # Update Critics And Exploration Model
            # TODO (Done): Update the exploration model (based off s')
            expl_model_loss = self.exploration_model.update(next_ob_no)

            # TODO (Done): Update the exploration critic
            # (based off mixed_reward)
            expl_loss = self.exploration_critic.update(
                ob_no=ob_no, ac_na=ac_na, next_ob_no=next_ob_no,
                reward_n=mixed_reward, terminal_n=terminal_n
            )
            # TODO (Done): Update the exploitation critic
            # (based off env_reward)
            expt_loss = self.exploitation_critic.update(
                ob_no=ob_no, ac_na=ac_na, next_ob_no=next_ob_no,
                reward_n=env_reward, terminal_n=terminal_n
            )

            # Target Networks #
            if self.num_param_updates % self.target_update_freq == 0:
                # NOTE: Update the exploitation and exploration target networks
                self.exploration_critic.update_target_network()
                self.exploitation_critic.update_target_network()

            # Logging #
            log['Exploitation Critic Loss'] = expt_loss['Training Loss']
            log['Exploration Critic Loss'] = expl_loss['Training Loss']
            log['Exploration Model Loss'] = expl_model_loss

            # Uncomment these lines after completing cql_critic.py
            if self.exploitation_critic.cql_alpha >= 0:
                log['Exploitation Data q-values'] = expt_loss['Data q-values']
                log['Exploitation OOD q-values'] = expt_loss['OOD q-values']
                log['Exploitation CQL Loss'] = expt_loss['CQL Loss']

            self.num_param_updates += 1

        self.t += 1
        return log

    def step_env(self):
        """Step the env and store the transition

        At the end of this block of code, the simulator should have been
        advanced one step, and the replay buffer should contain one more
        transition. Note that self.last_obs must always point to the new
        latest observation.
        """
        if (not self.offline_exploitation) or \
                (self.t <= self.num_exploration_steps):
            self.replay_buffer_idx = self.replay_buffer.store_frame(
                self.last_obs)

        perform_random_action = np.random.random(
        ) < self.eps or self.t < self.learning_starts

        if perform_random_action:
            action = self.env.action_space.sample()
        else:
            processed = self.replay_buffer.encode_recent_observation()
            action = self.actor.get_action(processed)

        next_obs, reward, done, info = self.env.step(action)
        self.last_obs = next_obs.copy()

        if (not self.offline_exploitation) or \
                (self.t <= self.num_exploration_steps):
            self.replay_buffer.store_effect(
                self.replay_buffer_idx, action, reward, done)

        if done:
            self.last_obs = self.env.reset()
